{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIMA: Neural Image Assessment\n",
    "replicate model for NIMA image assessment\n",
    "see: \n",
    "* https://research.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html\n",
    "* https://arxiv.org/abs/1709.05424\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "<a href=\"#Install\">Installation and setup</a><br>\n",
    "<a href=\"#Conversion\">`TFRecord` Conversion</a><br>\n",
    "<a href=\"#Model\">Model & Loss</a><br>\n",
    "<a href=\"#Train\">Training</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Components\n",
    "tensorflow models for VGG16, Inception-v2, MobileNet\n",
    "* https://github.com/tensorflow/models/tree/master/research/slim\n",
    "\n",
    "training datasets\n",
    "* AVA: A largescale database for aesthetic visual analysis\n",
    "  * http://refbase.cvc.uab.es/files/MMP2012a.pdf\n",
    "  * https://github.com/mtobeiyf/ava_downloader\n",
    "  * https://mega.nz/#F!hIEhQTLY key `!RkOnZv8Fz7EbYreHsiEzvA` (32GB)\n",
    "  * https://mega.nz/#!MUcXyBSB key `!0Q0Nq8_zBuSGiKmEHuKXKoAg8SDsB-21GwlJ22AJegU`\n",
    "  \n",
    "* TID2013: http://www.ponomarenko.info/tid2013.htm\n",
    "  * http://www.ponomarenko.info/tid2013/tid2013.rar (1GB)\n",
    "\n",
    "### Pipeline\n",
    "* input images are rescaled to 256 × 256, and then a crop of size 224 × 224 crop is randomly extracted.\n",
    "* random data augmentation in our training process is horizontal flipping of the image crops.\n",
    "\n",
    "### Score\n",
    "* mean quality score = `sum_N( s_i*p_i)`\n",
    "\n",
    "\n",
    "### Loss function\n",
    "* EMD (Earth Movers Distance) penalize mis-classifications according to class distances.\n",
    "  * https://gist.github.com/mjdietzx/a8121604385ce6da251d20d018f9a6d6\n",
    "  * https://www.tensorflow.org/api_docs/python/tf/distributions/Distribution\n",
    "* CMD, cumulative distr function, N_ava=10, N_tid=9\n",
    "    ```\n",
    "    EMD(p,phat) = (1/N.*sum_k( abs(CDF_p(k)-CDF_phat(k)).^2 )).^0.5\n",
    "    ```\n",
    "\n",
    "### Training\n",
    "* 80/20 train/test split on AVA and TID datasets\n",
    "\n",
    "hyperparameters\n",
    "  * `momentum=0.9, lambda=3e-7` \n",
    "  * `dropout=0.75` applied to last layer of baseline network\n",
    "\n",
    "FC layer, n=10, followed by softmax activations\n",
    "  * `lambda_fc=3e-6`\n",
    "\n",
    "lambda `decay=0.95` after every 10 epochs\n",
    "\n",
    "**???: how many epochs**\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Install'></a>\n",
    "## Installation and Setup\n",
    "\n",
    "### Download datasets\n",
    "* AVA dataset is 32GB, 256K images\n",
    "* TID dataset is about 1GB, about 3K images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set key paths\n",
    "import os\n",
    "if not 'HOME' in globals(): \n",
    "    HOME = %pwd\n",
    "SLIM = HOME + '/models/research/slim'\n",
    "CHECKPOINTS = os.path.join(HOME, 'ckpt')\n",
    "TRAIN_LOG = os.path.join(HOME, 'log')\n",
    "TMP = HOME + '/tmp'\n",
    "TID=os.path.join(HOME, 'data', 'tid')\n",
    "AVA=os.path.join(HOME, 'data', 'ava')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir -p $AVA\n",
    "# !mkdir -p $TID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA\n",
    "# download tid, 1GB \n",
    "!wget http://www.ponomarenko.info/tid2013/tid2013.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rar archiver for python/conda, https://anaconda.org/pypi/unrar\n",
    "!pip install -i https://pypi.anaconda.org/pypi/simple unrar\n",
    "#!conda install -c mlgill rarfile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVA dataset\n",
    "The AVA dataset is available on MEGA.nz as a 32GB download split into 64 `7z` archive files. You must first register with MEGA and install the desktop client in order to get enough transfer bandwidth to download. Overall, it will take a few days. see https://github.com/mtobeiyf/ava_downloader.\n",
    "\n",
    "Once the archive files are available, use a `7z` unarchiver to extract. The dataset is 255,000 JPG files in **one** directory.\n",
    "\n",
    "The images are converted into `TF_Records` for learning. For NIMA, an optional step is to resize all images to `(256,256,3)` before creating `TF_Records` to minimize upload times for cloud=based training. On OSX, this can be done via the following shell script:\n",
    "\n",
    "```\n",
    "  export SOURCE=/Volumes/data/DATASETS/AVA/images\n",
    "  export TARGET=/Volumes/data/DATASETS/AVA/images-256\n",
    "  mkdir -p $TARGET\n",
    "  for f in $SOURCE/*.jpg; do\n",
    "    sips -z 256 256 --setProperty formatOptions high $f --out $TARGET\n",
    "  done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Models with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $HOME\n",
    "!git clone https://github.com/mixuala/models  # https://github.com/tensorflow/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check tf-slim install\n",
    "%cd $SLIM\n",
    "!python -c \"from nets import cifarnet; mynet = cifarnet.cifarnet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $SLIM\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download tensorflow checkpoints, i.e. pre-trained weights\n",
    "!mkdir -p $CKPT\n",
    "!mkdir -p $TMP\n",
    "%cd $TMP\n",
    "\n",
    "### download model checkpoint\n",
    "# vgg16\n",
    "!wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n",
    "!tar -xvf vgg_16_2016_08_28.tar.gz\n",
    "%mv vgg_16.ckpt $CKPT\n",
    "%rm vgg_16_2016_08_28.tar.gz\n",
    "\n",
    "# inception-v2\n",
    "!wget http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz\n",
    "!tar -xvf inception_v2_2016_08_28.tar.gz\n",
    "%mv inception_v2.ckpt $CKPT\n",
    "%rm inception_v2_2016_08_28.tar.gz\n",
    "\n",
    "# MobileNet\n",
    "!wget http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "!tar -xvf mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "%mv mobilenet_v1_1.0_224.ckpt.* $CKPT\n",
    "%rm mobilenet_v1_1.0_224_2017_06_14.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Conversion'></a>\n",
    "## Dataset Conversion to `TFRecord`\n",
    "\n",
    "* Choosing shard values to get TFRecord files of size ~100MB see: https://www.tensorflow.org/performance/performance_guide#input_pipeline_optimization\n",
    "* 20% of dataset reserved for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config env\n",
    "import os\n",
    "if not 'HOME' in globals(): \n",
    "    HOME = %pwd\n",
    "SLIM = HOME + '/models/research/slim'\n",
    "CHECKPOINTS = os.path.join(HOME, 'ckpt')\n",
    "TMP = HOME + '/tmp'\n",
    "TID = os.path.join(HOME, 'data', 'tid')\n",
    "AVA = os.path.join(HOME, 'data', 'ava')   # dev dataset  \"/snappi.ai/tensorflow/nima/data/ava\"\n",
    "# AVA = \"/Volumes/data/DATASETS/AVA\"        # 32GB dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataset to `TFRecord`\n",
    "%cd $SLIM\n",
    "import tensorflow as tf\n",
    "from datasets import dataset_utils, convert_nima_tid, convert_nima_ava\n",
    "\n",
    "# convert_nima_tid.run(TID)\n",
    "# convert_nima_ava.run(AVA, resized=True, shards=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify conversion by checking sample data\n",
    "# NOTE: _mean_image_subtraction() will cause color shifts\n",
    "%cd $SLIM\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from datasets import dataset_utils, nima_tid, nima_ava\n",
    "import tensorflow as tf\n",
    "from preprocessing import preprocessing_factory\n",
    "import preprocessing.nima_preprocessing as nima_pre\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import sys\n",
    "\n",
    "use_resized_images = False\n",
    "is_training = True\n",
    "split_name = 'train' if is_training else 'validation'\n",
    "\n",
    "nima_preprocessing = preprocessing_factory.get_preprocessing('nima')\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "    dataset_name = \"AVA\"\n",
    "    if dataset_name == \"AVA\":\n",
    "        dataset = nima_ava.get_split(split_name, AVA, \n",
    "                                     resized=use_resized_images)\n",
    "        data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "        image, id, ratings, mean, stddev, height, width = data_provider.get(\n",
    "            ['image', 'id', 'ratings', 'mean', 'stddev', 'height', 'width'])\n",
    "\n",
    "    elif dataset_name == \"TID\":\n",
    "        dataset = nima_tid.get_split(split_name, TID, file_pattern='nima_tid_%s_*.tfrecord')\n",
    "        data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "        image, id, mean, stddev, height, width = data_provider.get(\n",
    "            ['image', 'id', 'mean', 'stddev', 'height', 'width'])\n",
    "    else:\n",
    "        exit\n",
    "    \n",
    "    \n",
    "    # apply preprocessing\n",
    "    image = nima_preprocessing(image, 224,224,\n",
    "            is_training=is_training,\n",
    "            resized=use_resized_images)\n",
    "\n",
    "        \n",
    "    with tf.Session() as sess:    \n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in range(4):\n",
    "                if dataset_name==\"AVA\":\n",
    "                    np_image, np_id, np_mean, np_stddev, np_ratings, np_h, np_w = sess.run(\n",
    "                        [image, id, mean, stddev, ratings, height, width])\n",
    "                    title = '%s: %d x %d, %f/%f, (%s) %s' % (\n",
    "                        np_id.decode(\"utf-8\"), np_h, np_w, np_mean, np_stddev, np_ratings, tf.shape(image))\n",
    "                else:\n",
    "                    np_image, np_id, np_mean, np_stddev, np_h, np_w = sess.run(\n",
    "                        [image, id, mean, stddev, height, width])\n",
    "                    title = '%s: %d x %d, %f/%f, %s' % (\n",
    "                        np_id.decode(\"utf-8\"), np_h, np_w, np_mean, np_stddev, tf.shape(image))\n",
    "                    \n",
    "                h,w, _ = np_image.shape\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(np_image.astype(np.uint8))\n",
    "                plt.title(title)\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Model'></a>\n",
    "## Model & Loss\n",
    "Nima was developed to work with multiple pre-trained CNNs, including `Vgg16`, `Inception-v2`, and `MobileNet`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "# from datasets.nima import load_batch\n",
    "\n",
    "\n",
    "# modified from slim_walkthrough\n",
    "%cd $SLIM\n",
    "import tensorflow as tf\n",
    "from preprocessing import preprocessing_factory\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "nima_preprocessing = preprocessing_factory.get_preprocessing('nima')\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=224, width=224, \n",
    "            is_training=False, \n",
    "            resized=True,\n",
    "            model=\"vgg16\",\n",
    "            label_name=\"ratings\"):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "      resized: Whether the TFRecords were converted with images already resized to (256,256,3)\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', label_name])\n",
    "    \n",
    "    # Preprocess image for usage by the appropriate model.\n",
    "    image = {\n",
    "      'vgg16': nima_preprocessing(image_raw, height, width, is_training=is_training,\n",
    "                              resized=resized),\n",
    "      'inception': None,\n",
    "      'mobilenet': None,\n",
    "    }[model]\n",
    "\n",
    "        \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nima model specifies a different `learning rate` for the `finetune` layers of the model, and also a `momentum` value, which was not part of the original `vgg_16.ckpt`. This seems to suggest that we need to use 2 separate optimizers with 2 separate gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.training import training_util\n",
    "\n",
    "def slim_learning_create_train_op_with_manual_grads( total_loss, optimizers, grads_and_vars,\n",
    "            global_step=0,                                                            \n",
    "#                     update_ops=None,\n",
    "#                     variables_to_train=None,\n",
    "            clip_gradient_norm=0,\n",
    "            summarize_gradients=False,\n",
    "            gate_gradients=1,               # tf.python.training.optimizer.Optimizer.GATE_OP,\n",
    "            aggregation_method=None,\n",
    "            colocate_gradients_with_ops=False,\n",
    "            gradient_multipliers=None,\n",
    "            check_numerics=True):\n",
    "    \"\"\"Runs the training loop\n",
    "            modified from slim.learning.create_train_op() to work with\n",
    "            a matched list of optimizers and grads_and_vars\n",
    "\n",
    "    Returns:\n",
    "        train_ops - the value of the loss function after training.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform_grads_fn(grads):\n",
    "        if gradient_multipliers:\n",
    "            with ops.name_scope('multiply_grads'):\n",
    "                grads = multiply_gradients(grads, gradient_multipliers)\n",
    "\n",
    "        # Clip gradients.\n",
    "        if clip_gradient_norm > 0:\n",
    "            with ops.name_scope('clip_grads'):\n",
    "                grads = clip_gradient_norms(grads, clip_gradient_norm)\n",
    "        return grads\n",
    "\n",
    "    if global_step is None:\n",
    "        global_step = training_util.get_or_create_global_step()\n",
    "\n",
    "    assert len(optimizers)==len(grads_and_vars)\n",
    "\n",
    "    ### order of processing:\n",
    "    # 0. grads = opt.compute_gradients() \n",
    "    # 1. grads = transform_grads_fn(grads)\n",
    "    # 2. add_gradients_summaries(grads)\n",
    "    # 3. grads = opt.apply_gradients(grads, global_step=global_step) \n",
    "\n",
    "    grad_updates = []\n",
    "    for i in range(len(optimizers)):\n",
    "        grads = grads_and_vars[i]                               # 0. kvarg, from opt.compute_gradients()\n",
    "        grads = transform_grads_fn(grads)                       # 1. transform_grads_fn()\n",
    "        if summarize_gradients:\n",
    "            with ops.name_scope('summarize_grads'):\n",
    "                slim.learning.add_gradients_summaries(grads)    # 2. add_gradients_summaries()\n",
    "        if i==0:\n",
    "            grad_update = optimizers[i].apply_gradients( grads, # 3. optimizer.apply_gradients()\n",
    "                        global_step=global_step)                #    update global_step only once\n",
    "        else:\n",
    "            grad_update = optimizers[i].apply_gradients( grads )\n",
    "        grad_updates.append(grad_update)\n",
    "\n",
    "    with ops.name_scope('train_op'):\n",
    "        total_loss = array_ops.check_numerics(total_loss,\n",
    "                                        'LossTensor is inf or nan')\n",
    "        train_op = control_flow_ops.with_dependencies(grad_updates, total_loss)\n",
    "\n",
    "    # Add the operation used for training to the 'train_op' collection    \n",
    "    train_ops = ops.get_collection_ref(ops.GraphKeys.TRAIN_OP)\n",
    "    if train_op not in train_ops:\n",
    "        train_ops.append(train_op)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMD loss function\n",
    "\n",
    "The loss function uses a normalized `Earth Movers Distance` to penalize mis-classifications according to class distances. This is based on the difference between the `Cumulative Distribution Function (CDF)` of the `y` and `y_hat` values for the ratings distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd $HOME\n",
    "# from nima_utils import NimaUtils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" _CDF in tensorflow \"\"\"\n",
    "#\n",
    "# private methods used by class NimaUtils()\n",
    "#\n",
    "def _weighted_score(x):\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  return tf.multiply(x, tf.range(1, n+1 , dtype=tf.float32))  # (None,10)\n",
    "\n",
    "def _CDF (k, x):\n",
    "  # assert k <= tf.shape(x)[1]\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  w_score = _weighted_score(x)        # (None,10)\n",
    "  cum_k_score = tf.reduce_sum(w_score[:,:k], axis=1)  # (None)\n",
    "  total = tf.reduce_sum(w_score, axis=1)  # (None)\n",
    "  cdf = tf.divide(cum_k_score, total)     # (None)\n",
    "  return tf.reshape(cdf, [m,1] ) # (None,1)\n",
    "\n",
    "def _cum_CDF (x):\n",
    "  # y = tf.concat( [   _CDF(i,x)    for i in tf.range(1, tf.shape(x)[1]+1) ] )\n",
    "  x = tf.to_float(x)\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  y = tf.concat( [_CDF(1,x),_CDF(2,x),_CDF(3,x),_CDF(4,x),_CDF(5,x),\n",
    "      _CDF(6,x),_CDF(7,x),_CDF(8,x),_CDF(9,x),_CDF(10,x)], \n",
    "      axis=1 )\n",
    "  return tf.reshape(y, [m,n] )\n",
    "\n",
    "def _emd(y, y_hat):\n",
    "    \"\"\"Returns the earth mover distance between to arrays of ratings, \n",
    "    based on cumulative distribution function\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      float \n",
    "    \"\"\"\n",
    "    r = 2.\n",
    "    m,n = tf.convert_to_tensor(y).get_shape().as_list()\n",
    "    N = tf.to_float(n)\n",
    "    cdf_loss = tf.subtract(_cum_CDF(y), _cum_CDF(y_hat))\n",
    "    emd_loss = tf.pow( tf.divide( tf.reduce_sum( tf.pow(cdf_loss, r), axis=1 ), N), 1/r)\n",
    "  #   return tf.reshape(emd_loss, [m,1])\n",
    "    return tf.reduce_mean(emd_loss)\n",
    "\n",
    "\n",
    "class NimaUtils(object):\n",
    "  \"\"\"Help Class for Nima calculations\n",
    "    NimaUtils.emd(y, y_hat) return float\n",
    "    NimaUtils.score( y ) returns [[mean, std]]\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def emd(y, y_hat):\n",
    "    return _emd(y, y_hat)\n",
    "\n",
    "  @staticmethod\n",
    "  def mu(y, shape=None):\n",
    "    \"\"\"mean quality score for ratings\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      array of [mean] floats for each row in y\n",
    "    \"\"\"\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    m,n = y.get_shape().as_list()\n",
    "    mean = tf.reduce_sum(_weighted_score(y), axis=1)/tf.reduce_sum(y, axis=1)\n",
    "    return tf.reshape(mean, [m,1])\n",
    "  \n",
    "  @staticmethod\n",
    "  def sigma(y, shape=None):\n",
    "    \"\"\"standard deviation of ratings\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      array of [stddev] floats for each row in y\n",
    "    \"\"\"    \n",
    "    y = tf.convert_to_tensor(y)\n",
    "    m,n = y.get_shape().as_list()    \n",
    "    mean = mu(y)\n",
    "    s = tf.range(1, n+1 , dtype=tf.float32)\n",
    "    p_score = tf.divide(y, tf.reshape(tf.reduce_sum(y, axis=1),[m,1]))\n",
    "    stddev = tf.sqrt(tf.reduce_sum( tf.multiply(tf.square(tf.subtract(s,mean)),p_score), axis=1))\n",
    "    return tf.reshape(stddev, [m,1])\n",
    "\n",
    "  @staticmethod\n",
    "  def score(y):\n",
    "    \"\"\"returns [mean quality score, stddev] for each row\"\"\"\n",
    "    return tf.concat([mu(y), sigma(y)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "h_params = {\n",
    "    \"regularization\": \"?\",\n",
    "    \"momentum\": 0.9,    # weight and bias   \n",
    "    \"dropout_keep\": 0.75,    # applied to last layer of baseline network only, scope=\"dropout7\"\n",
    "    \"learning_rate\": {  \n",
    "        \"baseline\": 3e-7,    # baseline CNN layers\n",
    "        \"finetune\": 3e-6,    # last fc layer only\n",
    "    },\n",
    "    \"learning_rate_decay\": 0.95,  # applied to both learning rates, after every 10 epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    'ava':{\n",
    "        'path': AVA,\n",
    "        'max_score': 10\n",
    "    },\n",
    "    'tid':{\n",
    "        'path': TID,\n",
    "        'max_score': 9\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vgg16\n",
    "Finetune `Vgg16` for `Nima`, using AVA dataset\n",
    "* the `baseline` model is `Vgg16` with the top (fc8) layer removed\n",
    "* `finetune` with a `fully_connected` layer with `softmax` activations, `n=10` for AVA\n",
    "* **???:** `n=9` for TID2013. TID2013 is based on ratings from 1-9, but these score distributions are approximated from a mean/stddev target value using `maximum entropy optimization`\n",
    "* remove `Vgg16/fc8` and add `fc8` for 10 classes to learn ratings\n",
    "* restore `vgg_16.ckpt` weights for all but last layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "# Nima Model based on Vgg16 for training against AVA dataset\n",
    "%cd $SLIM\n",
    "from nets import vgg\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "def nima_vgg16(inputs, dataset_params, h_params):\n",
    "    #\n",
    "    # load the model, use the default arg scope to configure the batch norm parameters.\n",
    "    #\n",
    "    net = { \n",
    "        \"baseline\": \"baseline image classifier with last layer removed\", \n",
    "        \"finetune\": \"finetuning layer with softmax activations, n=10 for AVA, 9 for TID2013\"\n",
    "    }\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "        # define baseline network with last layer removed\n",
    "        net[\"baseline\"], end_points = vgg.vgg_16(inputs, \n",
    "#                                      dropout_keep_prob=0.5,\n",
    "                                     num_classes=None,\n",
    "                                     )\n",
    "        \n",
    "        # define finetuning network, dropout, fc, softmax\n",
    "        num_classes=dataset_params['ava']['max_score']\n",
    "        dropout_keep_prob=h_params[\"dropout_keep\"]\n",
    "        net[\"finetune\"] = slim.dropout( net[\"baseline\"], dropout_keep_prob,\n",
    "                                       is_training=is_training,\n",
    "                                       scope='nima/dropout7' )\n",
    "        net[\"finetune\"] = slim.fully_connected( net[\"finetune\"], num_classes,\n",
    "                                               activation_fn=tf.nn.softmax,\n",
    "                                               scope='nima/fc8' )\n",
    "        end_points[\"nima/fc8\"] = net[\"finetune\"] = tf.squeeze( net[\"finetune\"], [1, 2], name='nima/fc8/squeezed')\n",
    "    return net, end_points\n",
    "\n",
    "#     predictions = net[\"finetune\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Train'></a>\n",
    "## Training\n",
    "Using `tf.slim.learning`-style training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima\n",
      "/snappi.ai/tensorflow/nima/models/research/slim\n",
      ">> TFRecord_dir=/snappi.ai/tensorflow/nima/data/ava/TFRecords_resized, \n",
      ">> pattern=nima_ava_train_*.tfrecord\n",
      "WARNING:tensorflow:Variable Variable missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable nima/fc8/weights missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable nima/fc8/biases missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv1/conv1_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv1/conv1_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc6/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc6/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc7/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc7/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable nima/fc8/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable nima/fc8/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /snappi.ai/tensorflow/nima/log/model.ckpt-200\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:Recording summary at step 200.\n",
      "INFO:tensorflow:Variable/sec: 0\n",
      "INFO:tensorflow:global step 201: loss = 0.8296 (53.901 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n"
     ]
    }
   ],
   "source": [
    "%cd $HOME\n",
    "# from nima_utils import NimaUtils\n",
    "\n",
    "%cd $SLIM\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from datasets import dataset_utils, nima_tid, nima_ava\n",
    "\n",
    "# training params\n",
    "TRAIN_LOG = os.path.join(HOME, 'log')\n",
    "checkpoints_dir = CHECKPOINTS\n",
    "log_dir = TRAIN_LOG\n",
    "use_resized_images = True\n",
    "is_training = True\n",
    "split_name = 'train' if is_training else 'validation'\n",
    "\n",
    "if not tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.MakeDirs(log_dir)\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    #\n",
    "    # prepare mini-batches\n",
    "    #\n",
    "    dataset = nima_ava.get_split(split_name, dataset_params[\"ava\"][\"path\"], resized=use_resized_images)\n",
    "    images, images_raw, labels = load_batch(dataset, \n",
    "                batch_size=32,\n",
    "                is_training=is_training,\n",
    "                resized=use_resized_images )\n",
    "\n",
    "\n",
    "    \n",
    "    #\n",
    "    # load the model, use the default arg scope to configure the batch norm parameters.\n",
    "    #\n",
    "    #     net = { \n",
    "    #         \"baseline\": \"baseline image classifier with last layer removed\", \n",
    "    #         \"finetune\": \"finetuning layer with softmax activations, n=10 for AVA, 9 for TID2013\"\n",
    "    #     }\n",
    "    #\n",
    "    net, end_points = nima_vgg16(images, dataset_params, h_params)\n",
    "    predictions = net[\"finetune\"]\n",
    "#     print(end_points)\n",
    "\n",
    "    \n",
    "    #\n",
    "    # define loss functions––include with slim.losses.get_total_loss()\n",
    "    #\n",
    "    emd_loss =  NimaUtils.emd(labels, predictions) \n",
    "    tf.losses.add_loss(emd_loss) # Letting TF-Slim know about the emd loss.\n",
    "    total_loss = tf.losses.get_total_loss( add_regularization_losses=True )\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # configure training loop MANUALLY & apply hyperparams:\n",
    "    #   - exponential_decay of learning_rate\n",
    "    #   - learning_rate by layers\n",
    "    # \n",
    "    # apply learning rate decay    \n",
    "    for k in h_params[\"learning_rate\"]:\n",
    "        h_params[\"learning_rate\"][k] = tf.train.exponential_decay( h_params[\"learning_rate\"][k],\n",
    "                                    global_step, 10, h_params[\"learning_rate_decay\"], staircase=True)\n",
    "\n",
    "        \n",
    "    #\n",
    "    # configure training loop MANUALLY, apply learning rates by layer\n",
    "    #\n",
    "    #   see: https://stackoverflow.com/questions/34945554/how-to-set-layer-wise-learning-rate-in-tensorflow\n",
    "    split_index = -2     # last layer weights & bias, count=2\n",
    "    training = {\"baseline\":{}, \"finetune\":{}}\n",
    "    # vars\n",
    "    trainable = tf.trainable_variables()\n",
    "    training[\"baseline\"][\"vars\"] = trainable[:split_index]\n",
    "    training[\"finetune\"][\"vars\"] = trainable[-split_index:]\n",
    "    # grads\n",
    "    gradients = tf.gradients( total_loss, trainable )\n",
    "    training[\"baseline\"][\"grads\"] = gradients[:split_index]\n",
    "    training[\"finetune\"][\"grads\"] = gradients[-split_index:]\n",
    "    # optimizers\n",
    "    \n",
    "    training[\"baseline\"][\"opt\"] = tf.train.GradientDescentOptimizer(\n",
    "                                    learning_rate=h_params[\"learning_rate\"][\"baseline\"])\n",
    "    # ???: Does this really work as I expect? \n",
    "    # I only want to apply momentum to the finetune layers, and the vgg_16.ckpt did not include momentum...\n",
    "    training[\"finetune\"][\"opt\"] = tf.train.MomentumOptimizer(\n",
    "                                    learning_rate=h_params[\"learning_rate\"][\"finetune\"],\n",
    "                                                   momentum=h_params[\"momentum\"]\n",
    "                                    )\n",
    "\n",
    "    grads_and_vars = [ zip(training[\"baseline\"][\"grads\"], training[\"baseline\"][\"vars\"]), \n",
    "                       zip(training[\"finetune\"][\"grads\"], training[\"finetune\"][\"vars\"]) ]\n",
    "    optimizers = [ training[\"baseline\"][\"opt\"], training[\"finetune\"][\"opt\"] ]\n",
    "\n",
    "    train_op = slim_learning_create_train_op_with_manual_grads(total_loss, \n",
    "                                                               optimizers, grads_and_vars, \n",
    "                                                               global_step=global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "    # restore Vgg16 to net[\"baseline\"], excludes last layer, fc8\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'vgg_16.ckpt'),\n",
    "        slim.get_variables_to_restore(exclude=['fc8']),\n",
    "        ignore_missing_vars=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # start training\n",
    "    slim.learning.train(train_op, log_dir, \n",
    "                        init_fn=init_fn,\n",
    "                        global_step=global_step,\n",
    "                        number_of_steps=1,\n",
    "                        save_summaries_secs=300,\n",
    "                        save_interval_secs=600                       \n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "I'm not sure if my work is correct because the loss does not seem to be decreasing. It seems the learning rate apecified by the paper is very low. But that may be the case for finetuning.\n",
    "\n",
    "example:\n",
    "\n",
    "```\n",
    "cwd= /snappi.ai/tensorflow/nima\n",
    "/snappi.ai/tensorflow/nima\n",
    "/snappi.ai/tensorflow/nima/models/research/slim\n",
    ">> TFRecord_dir=/snappi.ai/tensorflow/nima/data/ava/TFRecords_resized, \n",
    ">> pattern=nima_ava_train_*.tfrecord\n",
    "WARNING:tensorflow:Variable Variable missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
    "  ...\n",
    "WARNING:tensorflow:Variable nima/fc8/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
    "INFO:tensorflow:Restoring parameters from /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
    "INFO:tensorflow:Starting Session.\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Starting Queues.\n",
    "INFO:tensorflow:Recording summary at step 0.\n",
    "INFO:tensorflow:global step 1: loss = 0.2411 (52.944 sec/step)\n",
    "INFO:tensorflow:global step 2: loss = 0.2706 (47.458 sec/step)\n",
    "INFO:tensorflow:global step 3: loss = 0.2439 (46.986 sec/step)\n",
    "INFO:tensorflow:global step 4: loss = 0.2590 (46.905 sec/step)\n",
    "INFO:tensorflow:global step 5: loss = 0.2604 (46.879 sec/step)\n",
    "INFO:tensorflow:global step 6: loss = 0.2965 (47.169 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 6.\n",
    "INFO:tensorflow:global step 7: loss = 0.2557 (48.756 sec/step)\n",
    "INFO:tensorflow:global step 8: loss = 0.2793 (48.956 sec/step)\n",
    "INFO:tensorflow:global step 9: loss = 0.2354 (47.047 sec/step)\n",
    "INFO:tensorflow:global step 10: loss = 0.2429 (47.182 sec/step)\n",
    "INFO:tensorflow:global step 11: loss = 0.2738 (47.103 sec/step)\n",
    "INFO:tensorflow:global step 12: loss = 0.2293 (47.438 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 12.\n",
    "INFO:tensorflow:global step 13: loss = 0.2434 (48.358 sec/step)\n",
    "INFO:tensorflow:global step 14: loss = 0.2394 (46.977 sec/step)\n",
    "INFO:tensorflow:global step 15: loss = 0.2367 (47.118 sec/step)\n",
    "INFO:tensorflow:global step 16: loss = 0.2863 (46.911 sec/step)\n",
    "INFO:tensorflow:global step 17: loss = 0.2633 (46.842 sec/step)\n",
    "INFO:tensorflow:global step 18: loss = 0.2641 (47.154 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 18.\n",
    "INFO:tensorflow:global step 19: loss = 0.2886 (47.331 sec/step)\n",
    "INFO:tensorflow:global step 20: loss = 0.2568 (47.216 sec/step)\n",
    "INFO:tensorflow:global step 21: loss = 0.2487 (47.125 sec/step)\n",
    "INFO:tensorflow:global step 22: loss = 0.2566 (47.144 sec/step)\n",
    "INFO:tensorflow:global step 23: loss = 0.2630 (47.150 sec/step)\n",
    "INFO:tensorflow:global step 24: loss = 0.2882 (47.146 sec/step)\n",
    "INFO:tensorflow:global step 25: loss = 0.2494 (47.674 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 25.\n",
    "INFO:tensorflow:global step 26: loss = 0.2635 (48.129 sec/step)\n",
    "INFO:tensorflow:global step 27: loss = 0.3057 (46.899 sec/step)\n",
    "INFO:tensorflow:global step 28: loss = 0.2511 (46.734 sec/step)\n",
    "INFO:tensorflow:global step 29: loss = 0.2301 (46.627 sec/step)\n",
    "INFO:tensorflow:global step 30: loss = 0.2742 (46.537 sec/step)\n",
    "INFO:tensorflow:global step 31: loss = 0.2958 (46.652 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 31.\n",
    "INFO:tensorflow:global step 32: loss = 0.2851 (46.692 sec/step)\n",
    "INFO:tensorflow:global step 33: loss = 0.2647 (46.899 sec/step)\n",
    "INFO:tensorflow:global step 34: loss = 0.2555 (46.852 sec/step)\n",
    "INFO:tensorflow:global step 35: loss = 0.2908 (46.984 sec/step)\n",
    "INFO:tensorflow:global step 36: loss = 0.2725 (47.227 sec/step)\n",
    "INFO:tensorflow:global step 37: loss = 0.2378 (47.406 sec/step)\n",
    "INFO:tensorflow:global step 38: loss = 0.2399 (47.037 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 38.\n",
    "INFO:tensorflow:global step 39: loss = 0.2363 (48.432 sec/step)\n",
    "INFO:tensorflow:global step 40: loss = 0.2476 (46.969 sec/step)\n",
    "INFO:tensorflow:global step 41: loss = 0.2826 (46.525 sec/step)\n",
    "INFO:tensorflow:global step 42: loss = 0.2775 (46.459 sec/step)\n",
    "INFO:tensorflow:global step 43: loss = 0.2431 (46.489 sec/step)\n",
    "INFO:tensorflow:global step 44: loss = 0.3016 (46.677 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 44.\n",
    "INFO:tensorflow:global step 45: loss = 0.2622 (46.657 sec/step)\n",
    "INFO:tensorflow:global step 46: loss = 0.2686 (46.656 sec/step)\n",
    "INFO:tensorflow:global step 47: loss = 0.3073 (46.638 sec/step)\n",
    "INFO:tensorflow:global step 48: loss = 0.2798 (46.625 sec/step)\n",
    "INFO:tensorflow:global step 49: loss = 0.2584 (46.735 sec/step)\n",
    "INFO:tensorflow:global step 50: loss = 0.2532 (46.627 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 50.\n",
    "INFO:tensorflow:global step 51: loss = 0.2988 (47.766 sec/step)\n",
    "INFO:tensorflow:global step 52: loss = 0.2730 (46.951 sec/step)\n",
    "INFO:tensorflow:global step 53: loss = 0.2739 (46.550 sec/step)\n",
    "INFO:tensorflow:global step 54: loss = 0.2351 (46.631 sec/step)\n",
    "INFO:tensorflow:global step 55: loss = 0.2330 (46.575 sec/step)\n",
    "INFO:tensorflow:global step 56: loss = 0.3008 (46.410 sec/step)\n",
    "INFO:tensorflow:global step 57: loss = 0.2345 (46.577 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 57.\n",
    "INFO:tensorflow:global step 58: loss = 0.2517 (46.613 sec/step)\n",
    "INFO:tensorflow:global step 59: loss = 0.2318 (46.482 sec/step)\n",
    "INFO:tensorflow:global step 60: loss = 0.2236 (46.541 sec/step)\n",
    "INFO:tensorflow:global step 61: loss = 0.2356 (46.566 sec/step)\n",
    "INFO:tensorflow:global step 62: loss = 0.2926 (46.569 sec/step)\n",
    "INFO:tensorflow:global step 63: loss = 0.2860 (46.663 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 63.\n",
    "INFO:tensorflow:global step 64: loss = 0.2564 (47.574 sec/step)\n",
    "INFO:tensorflow:global step 65: loss = 0.2384 (46.611 sec/step)\n",
    "INFO:tensorflow:global step 66: loss = 0.2528 (46.527 sec/step)\n",
    "INFO:tensorflow:global step 67: loss = 0.3030 (46.552 sec/step)\n",
    "INFO:tensorflow:global step 68: loss = 0.2381 (46.524 sec/step)\n",
    "INFO:tensorflow:global step 69: loss = 0.2605 (46.387 sec/step)\n",
    "INFO:tensorflow:global step 70: loss = 0.2575 (46.646 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 70.\n",
    "INFO:tensorflow:global step 71: loss = 0.2514 (46.583 sec/step)\n",
    "INFO:tensorflow:global step 72: loss = 0.2616 (46.463 sec/step)\n",
    "INFO:tensorflow:global step 73: loss = 0.2593 (46.600 sec/step)\n",
    "INFO:tensorflow:global step 74: loss = 0.2676 (46.619 sec/step)\n",
    "INFO:tensorflow:global step 75: loss = 0.2774 (46.669 sec/step)\n",
    "INFO:tensorflow:global step 76: loss = 0.2359 (48.814 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 76.\n",
    "INFO:tensorflow:global step 77: loss = 0.2480 (48.683 sec/step)\n",
    "INFO:tensorflow:global step 78: loss = 0.2611 (48.136 sec/step)\n",
    "INFO:tensorflow:global step 79: loss = 0.2570 (46.677 sec/step)\n",
    "INFO:tensorflow:global step 80: loss = 0.2615 (46.352 sec/step)\n",
    "INFO:tensorflow:global step 81: loss = 0.2978 (47.083 sec/step)\n",
    "INFO:tensorflow:global step 82: loss = 0.2632 (46.399 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 82.\n",
    "INFO:tensorflow:global step 83: loss = 0.2473 (46.549 sec/step)\n",
    "INFO:tensorflow:global step 84: loss = 0.2491 (46.496 sec/step)\n",
    "INFO:tensorflow:global step 85: loss = 0.2741 (46.842 sec/step)\n",
    "INFO:tensorflow:global step 86: loss = 0.2594 (46.641 sec/step)\n",
    "INFO:tensorflow:global step 87: loss = 0.2812 (46.693 sec/step)\n",
    "INFO:tensorflow:global step 88: loss = 0.2529 (46.740 sec/step)\n",
    "INFO:tensorflow:global step 89: loss = 0.2584 (46.629 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 89.\n",
    "INFO:tensorflow:global step 90: loss = 0.2596 (47.670 sec/step)\n",
    "INFO:tensorflow:global step 91: loss = 0.2453 (47.042 sec/step)\n",
    "INFO:tensorflow:global step 92: loss = 0.2478 (46.811 sec/step)\n",
    "INFO:tensorflow:global step 93: loss = 0.2633 (46.666 sec/step)\n",
    "INFO:tensorflow:global step 94: loss = 0.2550 (47.038 sec/step)\n",
    "INFO:tensorflow:global step 95: loss = 0.2699 (46.861 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 95.\n",
    "INFO:tensorflow:global step 96: loss = 0.2561 (46.975 sec/step)\n",
    "INFO:tensorflow:global step 97: loss = 0.2732 (46.860 sec/step)\n",
    "INFO:tensorflow:global step 98: loss = 0.2523 (46.889 sec/step)\n",
    "INFO:tensorflow:global step 99: loss = 0.2449 (46.868 sec/step)\n",
    "INFO:tensorflow:global step 100: loss = 0.2529 (48.063 sec/step)\n",
    "INFO:tensorflow:global step 101: loss = 0.2441 (48.094 sec/step)\n",
    "INFO:tensorflow:global step 102: loss = 0.2421 (46.797 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 102.\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:global step 103: loss = 0.2573 (48.506 sec/step)\n",
    "INFO:tensorflow:global step 104: loss = 0.2563 (46.535 sec/step)\n",
    "INFO:tensorflow:global step 105: loss = 0.2886 (46.542 sec/step)\n",
    "INFO:tensorflow:global step 106: loss = 0.2864 (46.441 sec/step)\n",
    "INFO:tensorflow:global step 107: loss = 0.2621 (46.477 sec/step)\n",
    "INFO:tensorflow:global step 108: loss = 0.2334 (46.507 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 108.\n",
    "INFO:tensorflow:global step 109: loss = 0.2396 (46.706 sec/step)\n",
    "INFO:tensorflow:global step 110: loss = 0.2778 (46.587 sec/step)\n",
    "INFO:tensorflow:global step 111: loss = 0.2857 (46.772 sec/step)\n",
    "INFO:tensorflow:global step 112: loss = 0.2342 (46.642 sec/step)\n",
    "INFO:tensorflow:global step 113: loss = 0.2274 (46.854 sec/step)\n",
    "INFO:tensorflow:global step 114: loss = 0.2599 (46.784 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 114.\n",
    "INFO:tensorflow:global step 115: loss = 0.2395 (47.684 sec/step)\n",
    "INFO:tensorflow:global step 116: loss = 0.2639 (46.540 sec/step)\n",
    "INFO:tensorflow:global step 117: loss = 0.2711 (46.579 sec/step)\n",
    "INFO:tensorflow:global step 118: loss = 0.2284 (46.787 sec/step)\n",
    "INFO:tensorflow:global step 119: loss = 0.2465 (46.986 sec/step)\n",
    "INFO:tensorflow:global step 120: loss = 0.2572 (46.728 sec/step)\n",
    "INFO:tensorflow:global step 121: loss = 0.2249 (46.698 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 121.\n",
    "INFO:tensorflow:global step 122: loss = 0.2752 (46.561 sec/step)\n",
    "INFO:tensorflow:global step 123: loss = 0.2563 (46.660 sec/step)\n",
    "INFO:tensorflow:global step 124: loss = 0.2370 (46.818 sec/step)\n",
    "INFO:tensorflow:global step 125: loss = 0.2938 (46.732 sec/step)\n",
    "INFO:tensorflow:global step 126: loss = 0.2634 (46.920 sec/step)\n",
    "INFO:tensorflow:global step 127: loss = 0.2872 (46.627 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 127.\n",
    "INFO:tensorflow:global step 128: loss = 0.2540 (47.629 sec/step)\n",
    "INFO:tensorflow:global step 129: loss = 0.2590 (48.070 sec/step)\n",
    "INFO:tensorflow:global step 130: loss = 0.2224 (47.305 sec/step)\n",
    "INFO:tensorflow:global step 131: loss = 0.2426 (46.822 sec/step)\n",
    "INFO:tensorflow:global step 132: loss = 0.2783 (46.753 sec/step)\n",
    "INFO:tensorflow:global step 133: loss = 0.2514 (46.809 sec/step)\n",
    "INFO:tensorflow:global step 134: loss = 0.2502 (46.836 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 134.\n",
    "INFO:tensorflow:global step 135: loss = 0.2566 (47.107 sec/step)\n",
    "INFO:tensorflow:global step 136: loss = 0.2465 (47.012 sec/step)\n",
    "INFO:tensorflow:global step 137: loss = 0.2588 (47.115 sec/step)\n",
    "INFO:tensorflow:global step 138: loss = 0.2283 (47.062 sec/step)\n",
    "INFO:tensorflow:global step 139: loss = 0.2709 (46.848 sec/step)\n",
    "INFO:tensorflow:global step 140: loss = 0.2637 (47.212 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 140.\n",
    "INFO:tensorflow:global step 141: loss = 0.2839 (47.479 sec/step)\n",
    "INFO:tensorflow:global step 142: loss = 0.2624 (46.992 sec/step)\n",
    "INFO:tensorflow:global step 143: loss = 0.2373 (46.566 sec/step)\n",
    "INFO:tensorflow:global step 144: loss = 0.2567 (46.647 sec/step)\n",
    "INFO:tensorflow:global step 145: loss = 0.2655 (46.627 sec/step)\n",
    "INFO:tensorflow:global step 146: loss = 0.2429 (46.757 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 146.\n",
    "INFO:tensorflow:global step 147: loss = 0.2256 (46.796 sec/step)\n",
    "INFO:tensorflow:global step 148: loss = 0.2358 (46.966 sec/step)\n",
    "INFO:tensorflow:global step 149: loss = 0.2805 (47.077 sec/step)\n",
    "INFO:tensorflow:global step 150: loss = 0.2479 (47.119 sec/step)\n",
    "INFO:tensorflow:global step 151: loss = 0.2413 (47.154 sec/step)\n",
    "INFO:tensorflow:global step 152: loss = 0.2692 (47.452 sec/step)\n",
    "INFO:tensorflow:global step 153: loss = 0.2399 (48.899 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 153.\n",
    "INFO:tensorflow:global step 154: loss = 0.2430 (48.641 sec/step)\n",
    "INFO:tensorflow:global step 155: loss = 0.2650 (48.286 sec/step)\n",
    "INFO:tensorflow:global step 156: loss = 0.2597 (46.670 sec/step)\n",
    "INFO:tensorflow:global step 157: loss = 0.2479 (46.360 sec/step)\n",
    "INFO:tensorflow:global step 158: loss = 0.2526 (46.614 sec/step)\n",
    "INFO:tensorflow:global step 159: loss = 0.2642 (46.503 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 159.\n",
    "INFO:tensorflow:global step 160: loss = 0.2464 (46.415 sec/step)\n",
    "INFO:tensorflow:global step 161: loss = 0.2572 (46.646 sec/step)\n",
    "INFO:tensorflow:global step 162: loss = 0.2588 (46.315 sec/step)\n",
    "INFO:tensorflow:global step 163: loss = 0.2801 (46.375 sec/step)\n",
    "INFO:tensorflow:global step 164: loss = 0.2692 (46.635 sec/step)\n",
    "INFO:tensorflow:global step 165: loss = 0.2676 (46.785 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 165.\n",
    "INFO:tensorflow:global step 166: loss = 0.2570 (47.218 sec/step)\n",
    "INFO:tensorflow:global step 167: loss = 0.2452 (47.525 sec/step)\n",
    "INFO:tensorflow:global step 168: loss = 0.2619 (46.151 sec/step)\n",
    "INFO:tensorflow:global step 169: loss = 0.2715 (46.252 sec/step)\n",
    "INFO:tensorflow:global step 170: loss = 0.2813 (46.462 sec/step)\n",
    "INFO:tensorflow:global step 171: loss = 0.2525 (46.510 sec/step)\n",
    "INFO:tensorflow:global step 172: loss = 0.2742 (47.134 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 172.\n",
    "INFO:tensorflow:global step 173: loss = 0.2351 (48.038 sec/step)\n",
    "INFO:tensorflow:global step 174: loss = 0.2663 (47.943 sec/step)\n",
    "INFO:tensorflow:global step 175: loss = 0.2885 (51.293 sec/step)\n",
    "INFO:tensorflow:global step 176: loss = 0.2470 (47.242 sec/step)\n",
    "INFO:tensorflow:global step 177: loss = 0.2360 (46.514 sec/step)\n",
    "INFO:tensorflow:global step 178: loss = 0.2460 (46.257 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 178.\n",
    "INFO:tensorflow:global step 179: loss = 0.2464 (47.571 sec/step)\n",
    "INFO:tensorflow:global step 180: loss = 0.2763 (49.329 sec/step)\n",
    "INFO:tensorflow:global step 181: loss = 0.2708 (46.327 sec/step)\n",
    "INFO:tensorflow:global step 182: loss = 0.2671 (46.133 sec/step)\n",
    "INFO:tensorflow:global step 183: loss = 0.2594 (46.992 sec/step)\n",
    "INFO:tensorflow:global step 184: loss = 0.2418 (46.241 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 184.\n",
    "INFO:tensorflow:global step 185: loss = 0.2411 (46.249 sec/step)\n",
    "INFO:tensorflow:global step 186: loss = 0.2856 (46.189 sec/step)\n",
    "INFO:tensorflow:global step 187: loss = 0.2530 (46.418 sec/step)\n",
    "INFO:tensorflow:global step 188: loss = 0.2348 (46.300 sec/step)\n",
    "INFO:tensorflow:global step 189: loss = 0.2668 (46.452 sec/step)\n",
    "INFO:tensorflow:global step 190: loss = 0.2922 (46.487 sec/step)\n",
    "INFO:tensorflow:global step 191: loss = 0.2145 (46.411 sec/step)\n",
    "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
    "INFO:tensorflow:Recording summary at step 191.\n",
    "INFO:tensorflow:global step 192: loss = 0.2491 (47.197 sec/step)\n",
    "INFO:tensorflow:global step 193: loss = 0.2705 (47.039 sec/step)\n",
    "INFO:tensorflow:global step 194: loss = 0.2641 (46.245 sec/step)\n",
    "INFO:tensorflow:global step 195: loss = 0.2299 (46.284 sec/step)\n",
    "INFO:tensorflow:global step 196: loss = 0.2646 (46.379 sec/step)\n",
    "INFO:tensorflow:global step 197: loss = 0.2417 (46.250 sec/step)\n",
    "INFO:tensorflow:Recording summary at step 197.\n",
    "INFO:tensorflow:global step 198: loss = 0.2045 (46.297 sec/step)\n",
    "INFO:tensorflow:global step 199: loss = 0.2479 (46.342 sec/step)\n",
    "INFO:tensorflow:global step 200: loss = 0.2837 (46.310 sec/step)\n",
    "INFO:tensorflow:Stopping Training.\n",
    "INFO:tensorflow:Finished training! Saving model to disk.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
