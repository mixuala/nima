{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIMA: Neural Image Assessment\n",
    "replicate model for NIMA image assessment\n",
    "see: \n",
    "* https://research.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html\n",
    "* https://arxiv.org/abs/1709.05424\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "<a href=\"#Install\">Installation and setup</a><br>\n",
    "<a href=\"#Conversion\">`TFRecord` Conversion</a><br>\n",
    "<a href=\"#Model\">Model & Loss</a><br>\n",
    "<a href=\"#Train\">Training</a><br>\n",
    "<a href=\"#Evaluation\">Evaluation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Components\n",
    "tensorflow models for VGG16, Inception-v2, MobileNet\n",
    "* https://github.com/tensorflow/models/tree/master/research/slim\n",
    "\n",
    "training datasets\n",
    "* AVA: A largescale database for aesthetic visual analysis\n",
    "  * http://refbase.cvc.uab.es/files/MMP2012a.pdf\n",
    "  * https://github.com/mtobeiyf/ava_downloader\n",
    "  * https://mega.nz/#F!hIEhQTLY key `!RkOnZv8Fz7EbYreHsiEzvA` (32GB)\n",
    "  * https://mega.nz/#!MUcXyBSB key `!0Q0Nq8_zBuSGiKmEHuKXKoAg8SDsB-21GwlJ22AJegU`\n",
    "  \n",
    "* TID2013: http://www.ponomarenko.info/tid2013.htm\n",
    "  * http://www.ponomarenko.info/tid2013/tid2013.rar (1GB)\n",
    "\n",
    "### Pipeline\n",
    "* input images are rescaled to 256 × 256, and then a crop of size 224 × 224 crop is randomly extracted.\n",
    "* random data augmentation in our training process is horizontal flipping of the image crops.\n",
    "\n",
    "### Score\n",
    "* mean quality score = `sum_N( s_i*p_i)`\n",
    "\n",
    "\n",
    "### Loss function\n",
    "* EMD (Earth Movers Distance) penalize mis-classifications according to class distances.\n",
    "  * https://gist.github.com/mjdietzx/a8121604385ce6da251d20d018f9a6d6\n",
    "  * https://www.tensorflow.org/api_docs/python/tf/distributions/Distribution\n",
    "* CMD, cumulative distr function, N_ava=10, N_tid=9\n",
    "    ```\n",
    "    EMD(p,phat) = (1/N.*sum_k( abs(CDF_p(k)-CDF_phat(k)).^2 )).^0.5\n",
    "    ```\n",
    "\n",
    "### Training\n",
    "* 80/20 train/test split on AVA and TID datasets\n",
    "\n",
    "hyperparameters\n",
    "  * `momentum=0.9, lambda=3e-7` \n",
    "  * `dropout=0.75` applied to last layer of baseline network\n",
    "\n",
    "FC layer, n=10, followed by softmax activations\n",
    "  * `lambda_fc=3e-6`\n",
    "\n",
    "lambda `decay=0.95` after every 10 epochs\n",
    "\n",
    "**???: how many epochs**\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Install'></a>\n",
    "## Installation and Setup\n",
    "\n",
    "### Download datasets\n",
    "* AVA dataset is 32GB, 256K images\n",
    "* TID dataset is about 1GB, about 3K images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set key paths\n",
    "import os\n",
    "if not 'HOME' in globals(): \n",
    "    HOME = %pwd\n",
    "SLIM = HOME + '/models/research/slim'\n",
    "CHECKPOINTS = os.path.join(HOME, 'ckpt')\n",
    "TRAIN_LOG = os.path.join(HOME, 'log')\n",
    "TMP = HOME + '/tmp'\n",
    "TID=os.path.join(HOME, 'data', 'tid')\n",
    "AVA=os.path.join(HOME, 'data', 'ava')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir -p $AVA\n",
    "# !mkdir -p $TID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA\n",
    "# download tid, 1GB \n",
    "!wget http://www.ponomarenko.info/tid2013/tid2013.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rar archiver for python/conda, https://anaconda.org/pypi/unrar\n",
    "!pip install -i https://pypi.anaconda.org/pypi/simple unrar\n",
    "#!conda install -c mlgill rarfile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVA dataset\n",
    "The AVA dataset is available on MEGA.nz as a 32GB download split into 64 `7z` archive files. You must first register with MEGA and install the desktop client in order to get enough transfer bandwidth to download. Overall, it will take a few days. see https://github.com/mtobeiyf/ava_downloader.\n",
    "\n",
    "Once the archive files are available, use a `7z` unarchiver to extract. The dataset is 255,000 JPG files in **one** directory.\n",
    "\n",
    "The images are converted into `TF_Records` for learning. For NIMA, an optional step is to resize all images to `(256,256,3)` before creating `TF_Records` to minimize upload times for cloud=based training. On OSX, this can be done via the following shell script:\n",
    "\n",
    "```\n",
    "  export SOURCE=/Volumes/data/DATASETS/AVA/images\n",
    "  export TARGET=/Volumes/data/DATASETS/AVA/images-256\n",
    "  mkdir -p $TARGET\n",
    "  for f in $SOURCE/*.jpg; do\n",
    "    sips -z 256 256 --setProperty formatOptions high $f --out $TARGET\n",
    "  done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Models with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $HOME\n",
    "!git clone https://github.com/mixuala/models  # https://github.com/tensorflow/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check tf-slim install\n",
    "%cd $SLIM\n",
    "!python -c \"from nets import cifarnet; mynet = cifarnet.cifarnet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $SLIM\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download tensorflow checkpoints, i.e. pre-trained weights\n",
    "!mkdir -p $CKPT\n",
    "!mkdir -p $TMP\n",
    "%cd $TMP\n",
    "\n",
    "### download model checkpoint\n",
    "# vgg16\n",
    "!wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n",
    "!tar -xvf vgg_16_2016_08_28.tar.gz\n",
    "%mv vgg_16.ckpt $CKPT\n",
    "%rm vgg_16_2016_08_28.tar.gz\n",
    "\n",
    "# inception-v2\n",
    "!wget http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz\n",
    "!tar -xvf inception_v2_2016_08_28.tar.gz\n",
    "%mv inception_v2.ckpt $CKPT\n",
    "%rm inception_v2_2016_08_28.tar.gz\n",
    "\n",
    "# MobileNet\n",
    "!wget http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "!tar -xvf mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "%mv mobilenet_v1_1.0_224.ckpt.* $CKPT\n",
    "%rm mobilenet_v1_1.0_224_2017_06_14.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Conversion'></a>\n",
    "## Dataset Conversion to `TFRecord`\n",
    "\n",
    "* Choosing shard values to get TFRecord files of size ~100MB see: https://www.tensorflow.org/performance/performance_guide#input_pipeline_optimization\n",
    "* 20% of dataset reserved for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config env\n",
    "import os\n",
    "if not 'HOME' in globals(): \n",
    "    HOME = %pwd\n",
    "SLIM = HOME + '/models/research/slim'\n",
    "CHECKPOINTS = os.path.join(HOME, 'ckpt')\n",
    "TMP = HOME + '/tmp'\n",
    "TID = os.path.join(HOME, 'data', 'tid')\n",
    "AVA = os.path.join(HOME, 'data', 'ava')   # dev dataset  \"/snappi.ai/tensorflow/nima/data/ava\"\n",
    "# AVA = \"/Volumes/data/DATASETS/AVA\"        # 32GB dataset\n",
    "AVA = \"/Volumes/data/data/ava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataset to `TFRecord`\n",
    "%cd $SLIM\n",
    "import tensorflow as tf\n",
    "from datasets import dataset_utils, convert_nima_tid, convert_nima_ava\n",
    "AVA = \"/Volumes/data/data/ava\"\n",
    "# convert_nima_tid.run(TID)\n",
    "convert_nima_ava.run(AVA, resized=True, shards=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verify conversion by checking sample data\n",
    "# NOTE: _mean_image_subtraction() will cause color shifts\n",
    "%cd $SLIM\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from datasets import dataset_utils, nima_tid, nima_ava\n",
    "import tensorflow as tf\n",
    "from preprocessing import preprocessing_factory\n",
    "import preprocessing.nima_preprocessing as nima_pre\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import sys\n",
    "\n",
    "use_resized_images = True\n",
    "is_training = True\n",
    "split_name = 'train' if is_training else 'validation'\n",
    "\n",
    "nima_preprocessing = preprocessing_factory.get_preprocessing('nima')\n",
    "\n",
    "dataset_name = \"AVA\"\n",
    "with tf.Graph().as_default(): \n",
    "    \n",
    "    if dataset_name == \"AVA\":\n",
    "        dataset = nima_ava.get_split(split_name, AVA, \n",
    "                                     resized=use_resized_images)\n",
    "        data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "        image, id, ratings, mean, stddev, height, width = data_provider.get(\n",
    "            ['image', 'id', 'ratings', 'mean', 'stddev', 'height', 'width'])\n",
    "\n",
    "    elif dataset_name == \"TID\":\n",
    "        dataset = nima_tid.get_split(split_name, TID, file_pattern='nima_tid_%s_*.tfrecord')\n",
    "        data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "        image, id, mean, stddev, height, width = data_provider.get(\n",
    "            ['image', 'id', 'mean', 'stddev', 'height', 'width'])\n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    \n",
    "    # apply preprocessing\n",
    "    image = nima_preprocessing(image, 224,224,\n",
    "            is_training=is_training,\n",
    "            resized=use_resized_images)\n",
    "\n",
    "        \n",
    "    with tf.Session() as sess:    \n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in range(4):\n",
    "                if dataset_name==\"AVA\":\n",
    "                    np_image, np_id, np_mean, np_stddev, np_ratings, np_h, np_w = sess.run(\n",
    "                        [image, id, mean, stddev, ratings, height, width])\n",
    "                    title = '%s: %d x %d, %f/%f, (%s) %s' % (\n",
    "                        np_id.decode(\"utf-8\"), np_h, np_w, np_mean, np_stddev, np_ratings, tf.shape(image))\n",
    "                else:\n",
    "                    np_image, np_id, np_mean, np_stddev, np_h, np_w = sess.run(\n",
    "                        [image, id, mean, stddev, height, width])\n",
    "                    title = '%s: %d x %d, %f/%f, %s' % (\n",
    "                        np_id.decode(\"utf-8\"), np_h, np_w, np_mean, np_stddev, tf.shape(image))\n",
    "                    \n",
    "                h,w, _ = np_image.shape\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(np_image.astype(np.uint8))\n",
    "                plt.title(title)\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Model'></a>\n",
    "## Model & Loss\n",
    "Nima was developed to work with multiple pre-trained CNNs, including `Vgg16`, `Inception-v2`, and `MobileNet`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "# from datasets.nima import load_batch\n",
    "\n",
    "\n",
    "# modified from slim_walkthrough\n",
    "%cd $SLIM\n",
    "import tensorflow as tf\n",
    "from preprocessing import preprocessing_factory\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "nima_preprocessing = preprocessing_factory.get_preprocessing('nima')\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=224, width=224, \n",
    "            is_training=False, \n",
    "            resized=True,\n",
    "            model=\"vgg16\",\n",
    "            label_name=\"ratings\"):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "      resized: Whether the TFRecords were converted with images already resized to (256,256,3)\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', label_name])\n",
    "    \n",
    "    # Preprocess image for usage by the appropriate model.\n",
    "    image = {\n",
    "      'vgg16': nima_preprocessing(image_raw, height, width, is_training=is_training,\n",
    "                              resized=resized),\n",
    "      'inception': None,\n",
    "      'mobilenet': None,\n",
    "    }[model]\n",
    "\n",
    "        \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nima model specifies a different `learning rate` for the `finetune` layers of the model, and also a `momentum` value, which was not part of the original `vgg_16.ckpt`. This seems to suggest that we need to use 2 separate optimizers with 2 separate gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima\n"
     ]
    }
   ],
   "source": [
    "%cd $HOME\n",
    "# from nima_utils import slim_learning_create_train_op_with_manual_grads\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.training import training_util\n",
    "\n",
    "def slim_learning_create_train_op_with_manual_grads( total_loss, optimizers, grads_and_vars,\n",
    "            global_step=0,                                                            \n",
    "#                     update_ops=None,\n",
    "#                     variables_to_train=None,\n",
    "            clip_gradient_norm=0,\n",
    "            summarize_gradients=False,\n",
    "            gate_gradients=1,               # tf.python.training.optimizer.Optimizer.GATE_OP,\n",
    "            aggregation_method=None,\n",
    "            colocate_gradients_with_ops=False,\n",
    "            gradient_multipliers=None,\n",
    "            check_numerics=True):\n",
    "    \"\"\"Runs the training loop\n",
    "            modified from slim.learning.create_train_op() to work with\n",
    "            a matched list of optimizers and grads_and_vars\n",
    "\n",
    "    Returns:\n",
    "        train_ops - the value of the loss function after training.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform_grads_fn(grads):\n",
    "        if gradient_multipliers:\n",
    "            with ops.name_scope('multiply_grads'):\n",
    "                grads = multiply_gradients(grads, gradient_multipliers)\n",
    "\n",
    "        # Clip gradients.\n",
    "        if clip_gradient_norm > 0:\n",
    "            with ops.name_scope('clip_grads'):\n",
    "                grads = clip_gradient_norms(grads, clip_gradient_norm)\n",
    "        return grads\n",
    "\n",
    "    if global_step is None:\n",
    "        global_step = training_util.get_or_create_global_step()\n",
    "\n",
    "    assert len(optimizers)==len(grads_and_vars)\n",
    "\n",
    "    ### order of processing:\n",
    "    # 0. grads = opt.compute_gradients() \n",
    "    # 1. grads = transform_grads_fn(grads)\n",
    "    # 2. add_gradients_summaries(grads)\n",
    "    # 3. grads = opt.apply_gradients(grads, global_step=global_step) \n",
    "\n",
    "    grad_updates = []\n",
    "    for i in range(len(optimizers)):\n",
    "        grads = grads_and_vars[i]                               # 0. kvarg, from opt.compute_gradients()\n",
    "        grads = transform_grads_fn(grads)                       # 1. transform_grads_fn()\n",
    "        if summarize_gradients:\n",
    "            with ops.name_scope('summarize_grads'):\n",
    "                slim.learning.add_gradients_summaries(grads)    # 2. add_gradients_summaries()\n",
    "        if i==0:\n",
    "            grad_update = optimizers[i].apply_gradients( grads, # 3. optimizer.apply_gradients()\n",
    "                        global_step=global_step)                #    update global_step only once\n",
    "        else:\n",
    "            grad_update = optimizers[i].apply_gradients( grads )\n",
    "        grad_updates.append(grad_update)\n",
    "\n",
    "    with ops.name_scope('train_op'):\n",
    "        total_loss = array_ops.check_numerics(total_loss,\n",
    "                                        'LossTensor is inf or nan')\n",
    "        train_op = control_flow_ops.with_dependencies(grad_updates, total_loss)\n",
    "\n",
    "    # Add the operation used for training to the 'train_op' collection    \n",
    "    train_ops = ops.get_collection_ref(ops.GraphKeys.TRAIN_OP)\n",
    "    if train_op not in train_ops:\n",
    "        train_ops.append(train_op)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMD loss function\n",
    "\n",
    "The loss function uses a normalized `Earth Movers Distance` to penalize mis-classifications according to class distances. This is based on the difference between the `Cumulative Distribution Function (CDF)` of the `y` and `y_hat` values for the ratings distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %cd $HOME\n",
    "# from nima_utils import NimaUtils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" _CDF in tensorflow \"\"\"\n",
    "#\n",
    "# private methods used by class NimaUtils()\n",
    "#\n",
    "def _weighted_score(x):\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  return tf.multiply(x, tf.range(1, n+1 , dtype=tf.float32))  # (None,10)\n",
    "\n",
    "def _CDF (k, x):\n",
    "  # assert k <= tf.shape(x)[1]\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  w_score = _weighted_score(x)        # (None,10)\n",
    "  cum_k_score = tf.reduce_sum(w_score[:,:k], axis=1)  # (None)\n",
    "  total = tf.reduce_sum(w_score, axis=1)  # (None)\n",
    "  cdf = tf.divide(cum_k_score, total)     # (None)\n",
    "  return tf.reshape(cdf, [m,1] ) # (None,1)\n",
    "\n",
    "def _cum_CDF (x):\n",
    "  # y = tf.concat( [   _CDF(i,x)    for i in tf.range(1, tf.shape(x)[1]+1) ] )\n",
    "  x = tf.to_float(x)\n",
    "  m,n = tf.convert_to_tensor(x).get_shape().as_list()\n",
    "  y = tf.concat( [_CDF(1,x),_CDF(2,x),_CDF(3,x),_CDF(4,x),_CDF(5,x),\n",
    "      _CDF(6,x),_CDF(7,x),_CDF(8,x),_CDF(9,x),_CDF(10,x)], \n",
    "      axis=1 )\n",
    "  return tf.reshape(y, [m,n] )\n",
    "\n",
    "def _emd(y, y_hat):\n",
    "    \"\"\"Returns the earth mover distance between to arrays of ratings, \n",
    "    based on cumulative distribution function\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      float \n",
    "    \"\"\"\n",
    "    r = 2.\n",
    "    m,n = tf.convert_to_tensor(y).get_shape().as_list()\n",
    "    N = tf.to_float(n)\n",
    "    cdf_loss = tf.subtract(_cum_CDF(y), _cum_CDF(y_hat))\n",
    "    emd_loss = tf.pow( tf.divide( tf.reduce_sum( tf.pow(cdf_loss, r), axis=1 ), N), 1/r)\n",
    "  #   return tf.reshape(emd_loss, [m,1])\n",
    "    return tf.reduce_mean(emd_loss)\n",
    "\n",
    "\n",
    "class NimaUtils(object):\n",
    "  \"\"\"Help Class for Nima calculations\n",
    "    NimaUtils.emd(y, y_hat) return float\n",
    "    NimaUtils.score( y ) returns [[mean, std]]\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def emd(y, y_hat):\n",
    "    return _emd(y, y_hat)\n",
    "\n",
    "  @staticmethod\n",
    "  def mu(y, shape=None):\n",
    "    \"\"\"mean quality score for ratings\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      array of [mean] floats for each row in y\n",
    "    \"\"\"\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    m,n = y.get_shape().as_list()\n",
    "    mean = tf.reduce_sum(_weighted_score(y), axis=1)/tf.reduce_sum(y, axis=1)\n",
    "    return tf.reshape(mean, [m,1])\n",
    "  \n",
    "  @staticmethod\n",
    "  def sigma(y, shape=None):\n",
    "    \"\"\"standard deviation of ratings\n",
    "    \n",
    "    Args:\n",
    "      y, y_hat: a mini-batch of ratings, each composed of a count of scores \n",
    "                shape = (None, n), array of count of scores for score from 1..n\n",
    "\n",
    "    Returns:\n",
    "      array of [stddev] floats for each row in y\n",
    "    \"\"\"    \n",
    "    y = tf.convert_to_tensor(y)\n",
    "    m,n = y.get_shape().as_list()    \n",
    "    mean = NimaUtils.mu(y)\n",
    "    s = tf.range(1, n+1 , dtype=tf.float32)\n",
    "    p_score = tf.divide(y, tf.reshape(tf.reduce_sum(y, axis=1),[m,1]))\n",
    "    stddev = tf.sqrt(tf.reduce_sum( tf.multiply(tf.square(tf.subtract(s,mean)),p_score), axis=1))\n",
    "    return tf.reshape(stddev, [m,1])\n",
    "\n",
    "  @staticmethod\n",
    "  def score(y):\n",
    "    \"\"\"returns [mean quality score, stddev] for each row\"\"\"\n",
    "    return tf.concat([NimaUtils.mu(y), NimaUtils.sigma(y)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vgg16\n",
    "Finetune `Vgg16` for `Nima`, using AVA dataset\n",
    "* the `baseline` model is `Vgg16` with the top (fc8) layer removed\n",
    "* `finetune` with a `fully_connected` layer with `softmax` activations, `n=10` for AVA\n",
    "* **???:** `n=9` for TID2013. TID2013 is based on ratings from 1-9, but these score distributions are approximated from a mean/stddev target value using `maximum entropy optimization`\n",
    "* remove `Vgg16/fc8` and add `fc8` for 10 classes to learn ratings\n",
    "* restore `vgg_16.ckpt` weights for all but last layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima\n",
      "/snappi.ai/tensorflow/nima/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "# Nima Model based on Vgg16 for training against AVA dataset\n",
    "\n",
    "%cd $HOME\n",
    "# from nima_utils import nima_vgg_16\n",
    "\n",
    "%cd $SLIM\n",
    "from nets import vgg\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "# copied from nets.vgg.vgg_16 with slight modifications\n",
    "def nima_vgg_16(inputs,\n",
    "           num_classes=10,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           dropout7_keep_prob=0.5,        # added kvarg to change value for dropout7 only\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16',\n",
    "           fc_conv_padding='VALID',\n",
    "           global_pool=False):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes. If 0 or None, the logits layer is\n",
    "      omitted and the input features to the logits layer are returned instead.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output.\n",
    "      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n",
    "      'VALID' padding.\n",
    "    global_pool: Optional boolean flag. If True, the input to the classification\n",
    "      layer is avgpooled to size 1x1, for any input size. (This is not part\n",
    "      of the original VGG architecture.)\n",
    "\n",
    "  Returns:\n",
    "    net: the output of the logits layer (if num_classes is a non-zero integer),\n",
    "      or the input to the logits layer (if num_classes is 0 or None).\n",
    "    end_points: a dict of tensors with intermediate activations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if global_pool:\n",
    "        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "        end_points['global_pool'] = net\n",
    "      if num_classes:\n",
    "        net = slim.dropout(net, dropout7_keep_prob, is_training=is_training,  # override dropout_keep_prob\n",
    "                           scope='dropout7')\n",
    "        net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        if spatial_squeeze and num_classes is not None:\n",
    "          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "\n",
    "#     predictions = tf.nn.softmax(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and hyperparams\n",
    "dataset_params = {\n",
    "    'ava':{ 'path': AVA, 'max_score': 10 },\n",
    "    'tid':{ 'path': TID, 'max_score': 9  },\n",
    "    'is_training': True,\n",
    "    'use_resized_images': True,\n",
    "}\n",
    "# Hyperparams from research paper\n",
    "h_params = {\n",
    "    \"regularization\": \"?\",\n",
    "    \"momentum\": 0.9,         # weight and bias   \n",
    "    \"dropout7_keep\": 0.75,    # applied to last layer of baseline network only, scope=\"dropout7\"\n",
    "    \"learning_rate\": {  \n",
    "        \"baseline\": 3e-7,    # baseline CNN layers\n",
    "        \"finetune\": 3e-6,    # last fc layer only\n",
    "    },\n",
    "    \"learning_rate_decay\": 0.95,  # applied to both learning rates, after every 10 epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Train'></a>\n",
    "## Training\n",
    "Using `tf.slim.learning`-style training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima\n",
      "/snappi.ai/tensorflow/nima/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "# load helper functions (also defined above)\n",
    "%cd $HOME\n",
    "from nima_utils import NimaUtils, nima_vgg_16\n",
    "from nima_utils import slim_learning_create_train_op_with_manual_grads\n",
    "\n",
    "%cd $SLIM\n",
    "from datasets.nima import load_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# runtime adjustments\n",
    "checkpoints_dir = CHECKPOINTS\n",
    "log_dir = TRAIN_LOG = os.path.join(HOME, 'log')\n",
    "\n",
    "# learning adjustments\n",
    "dataset_params['is_training'] = True\n",
    "# h_params['learning_rate']['finetune'] = 3e-3\n",
    "dataset_name = \"ava\"  # ava or tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/snappi.ai/tensorflow/nima\n",
      "/snappi.ai/tensorflow/nima/models/research/slim\n",
      ">> TFRecord_dir=/snappi.ai/tensorflow/nima/data/ava/TFRecords_resized, \n",
      ">> pattern=nima_ava_train_*.tfrecord\n",
      "WARNING:tensorflow:Variable vgg_16/conv1/conv1_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv1/conv1_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv2/conv2_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv3/conv3_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv4/conv4_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_1/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_1/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_2/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_2/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_3/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/conv5/conv5_3/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc6/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc6/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc7/weights/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "WARNING:tensorflow:Variable vgg_16/fc7/biases/Momentum missing in checkpoint /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /snappi.ai/tensorflow/nima/ckpt/vgg_16.ckpt\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path /snappi.ai/tensorflow/nima/log/model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Variable/sec: 0\n",
      "INFO:tensorflow:global step 1: loss = 0.2510 (56.180 sec/step)\n"
     ]
    }
   ],
   "source": [
    "%cd $HOME\n",
    "# from nima_utils import NimaUtils\n",
    "\n",
    "# Vgg16 (last layer removed) > FC(10) > softmax activations > ratings predictions\n",
    "%cd $SLIM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocessing import preprocessing_factory\n",
    "from datasets import dataset_utils, nima_tid, nima_ava\n",
    "from datasets.nima import load_batch as load_nima_batch\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "if not tf.gfile.Exists(log_dir):  tf.gfile.MakeDirs(log_dir)\n",
    "    \n",
    "    \n",
    "# derived params\n",
    "split_name = 'train' if dataset_params[\"is_training\"] else 'validation'\n",
    "num_classes_finetune = dataset_params[dataset_name][\"max_score\"]\n",
    "dataset_path = dataset_params[dataset_name][\"path\"]\n",
    "\n",
    "\n",
    "# build graph    \n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    #\n",
    "    # prepare mini-batches\n",
    "    #\n",
    "    dataset = nima_ava.get_split(split_name, dataset_path, \n",
    "                                 resized=dataset_params['use_resized_images'])\n",
    "    images, images_raw, labels = load_batch(dataset, \n",
    "                batch_size=32,\n",
    "                is_training=dataset_params[\"is_training\"],\n",
    "                resized=dataset_params['use_resized_images'] )\n",
    "\n",
    "\n",
    "  \n",
    "        \n",
    "    # load vgg_16 with modified value for dropout7\n",
    "    net, end_points = nima_vgg_16(images, \n",
    "                                  num_classes=num_classes_finetune, \n",
    "                                  dropout7_keep_prob=h_params[\"dropout7_keep\"],\n",
    "                                  is_training=dataset_params[\"is_training\"])\n",
    "    predictions = tf.nn.softmax(net)\n",
    "\n",
    "\n",
    "    #\n",
    "    # define loss functions––include with slim.losses.get_total_loss()\n",
    "    #\n",
    "    emd_loss =  NimaUtils.emd(labels, predictions) \n",
    "    tf.losses.add_loss(emd_loss) # Letting TF-Slim know about the emd loss.\n",
    "    total_loss = tf.losses.get_total_loss( add_regularization_losses=True )\n",
    "\n",
    "    #\n",
    "    # configure training loop MANUALLY & apply hyperparams:\n",
    "    #   - exponential_decay of learning_rate\n",
    "    #   - learning_rate by layers\n",
    "    # \n",
    "    # apply learning rate decay\n",
    "    lr_decay = {}\n",
    "    decay = h_params[\"learning_rate_decay\"]\n",
    "    for k in [\"baseline\", \"finetune\"]:\n",
    "        lr_decay[k] = tf.train.exponential_decay( h_params['learning_rate'][k],\n",
    "                                    global_step, 10, \n",
    "                                    h_params[\"learning_rate_decay\"], \n",
    "                                    staircase=True)\n",
    "\n",
    "\n",
    "    #\n",
    "    # configure training loop MANUALLY, apply learning rates by layer\n",
    "    #\n",
    "    #   see: https://stackoverflow.com/questions/34945554/how-to-set-layer-wise-learning-rate-in-tensorflow\n",
    "    split_index = -2     # last layer weights & bias, count=2\n",
    "    training = {\"baseline\":{}, \"finetune\":{}}\n",
    "    # vars\n",
    "    trainable = tf.trainable_variables()\n",
    "    training[\"baseline\"][\"vars\"] = trainable[:split_index]\n",
    "    training[\"finetune\"][\"vars\"] = trainable[-split_index:]\n",
    "    # grads\n",
    "    gradients = tf.gradients( total_loss, trainable )\n",
    "    training[\"baseline\"][\"grads\"] = gradients[:split_index]\n",
    "    training[\"finetune\"][\"grads\"] = gradients[-split_index:]\n",
    "    # optimizers\n",
    "\n",
    "    training[\"baseline\"][\"opt\"] = tf.train.GradientDescentOptimizer(\n",
    "                                    learning_rate=lr_decay[\"baseline\"])\n",
    "    # I only want to apply momentum to the finetune layers, and the vgg_16.ckpt did not include momentum...\n",
    "#         training[\"finetune\"][\"opt\"] = tf.train.GradientDescentOptimizer(\n",
    "#                                         learning_rate=lr_decay[\"finetune\"],\n",
    "#                                         )\n",
    "    training[\"finetune\"][\"opt\"] = tf.train.MomentumOptimizer(\n",
    "                                    learning_rate=lr_decay[\"finetune\"],\n",
    "                                    momentum=h_params[\"momentum\"]\n",
    "                                    )\n",
    "\n",
    "    grads_and_vars = [ zip(training[\"baseline\"][\"grads\"], training[\"baseline\"][\"vars\"]), \n",
    "                       zip(training[\"finetune\"][\"grads\"], training[\"finetune\"][\"vars\"]) ]\n",
    "    optimizers = [ training[\"baseline\"][\"opt\"], training[\"finetune\"][\"opt\"] ]\n",
    "\n",
    "    train_op = slim_learning_create_train_op_with_manual_grads(total_loss, \n",
    "                               optimizers, grads_and_vars, \n",
    "                               global_step=global_step)\n",
    "\n",
    "    \n",
    "    restore_vars = slim.get_variables_to_restore(exclude=['vgg_16/fc8', 'Variable'])\n",
    "    \n",
    "    # restore Vgg16 to net[\"baseline\"], excludes last layer, fc8\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'vgg_16.ckpt'),\n",
    "        restore_vars,\n",
    "        ignore_missing_vars=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # start training\n",
    "    final_loss = slim.learning.train(train_op, log_dir, \n",
    "                        init_fn=init_fn,\n",
    "                        global_step=global_step,\n",
    "                        number_of_steps=25,\n",
    "                        save_summaries_secs=300,\n",
    "                        save_interval_secs=600                       \n",
    "                       )\n",
    "\n",
    "    print('Finished training. Last batch loss %f' % final_loss)\n",
    "    print('>   dataset=', dataset_path )\n",
    "    print('>   hyperparams=%s' % h_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training log, small dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
